{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "0e0d1c7ef0f381ce9c31735005e25185fd13b9c57d8e85878ff9ff982cb55e39"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from utils import *\n",
    "from network import CharRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/internet_archive_scifi_v3.txt') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars, encoding = tokenize(text[580:])  # scifi text starts from character no '580'\n",
    "\n",
    "# create training and validation data\n",
    "val_frac = 0.01  # 1% of entire dataset (~1.5M chars)\n",
    "val_idx = int(len(encoding)*(1-val_frac))\n",
    "data, val_data = encoding[:val_idx], encoding[val_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "model = CharRNN(tokens=chars).to(device)\n",
    "print(model)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=1/3, patience=8, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    checkpoint = torch.load('drive/MyDrive/char_rnn_ckpt.pth', map_location=device)\n",
    "    model = CharRNN(\n",
    "        tokens=checkpoint['tokens'],\n",
    "        n_hidden=checkpoint['n_hidden'],\n",
    "        n_layers=checkpoint['n_layers'],\n",
    "        ).to(device)\n",
    "    model.load_state_dict(checkpoint['model'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    scheduler.load_state_dict(checkpoint['scheduler'])\n",
    "\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    print(f'checkpoint found, training will start from epoch {start_epoch}\\n')\n",
    "    print(model)\n",
    "\n",
    "except:\n",
    "    start_epoch = 0\n",
    "    print(f'no checkpoint found, training will start from epoch {start_epoch}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 64\n",
    "batch_size = 256\n",
    "num_epochs = 64\n",
    "n_chars = len(model.chars)\n",
    "val_loss_min = np.Inf\n",
    "\n",
    "for epoch in range(start_epoch, num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "\n",
    "    h = model.init_hidden(batch_size)  # initialize hidden state\n",
    "    loop = tqdm(get_batches(data, batch_size, seq_length), total=9023)\n",
    "\n",
    "    for x, y in loop:\n",
    "        x = one_hot_encode(x, n_chars)  # one hot encode the data\n",
    "        inputs, targets = torch.from_numpy(x).to(device), torch.from_numpy(y).to(device)\n",
    "        # creates new variables for the hidden state, otherwise\n",
    "        # the optimizer would backprop through the entire training history\n",
    "        h = tuple([each.data for each in h])\n",
    "        model.zero_grad()  # zero out accumulated gradients\n",
    "        outputs, h = model(inputs, h)\n",
    "        loss = criterion(outputs, targets.view(batch_size*seq_length).long())\n",
    "        loss.backward()\n",
    "        # clip gradients, prevents exploding gradient problem in RNNs\n",
    "        nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=5)\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        loop.set_description(f'Epoch [{epoch+1:2d}/{num_epochs}]')\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        val_h = model.init_hidden(batch_size)\n",
    "        for x, y in get_batches(val_data, batch_size, seq_length):\n",
    "            x = one_hot_encode(x, n_chars)\n",
    "            inputs, targets = torch.from_numpy(x).to(device), torch.from_numpy(y).to(device)\n",
    "            outputs, val_h = model(inouts, val_h)\n",
    "            loss = criterion(outputs, targets.view(batch_size*seq_length).long())\n",
    "            val_loss += loss.item()\n",
    "\n",
    "\n",
    "    scheduler.step(val_loss)\n",
    "    tqdm.write(f'\\t\\ttrain_loss={train_loss}, val_loss={val_loss}')\n",
    "\n",
    "    # save the model if validation loss has decreased\n",
    "    if val_loss <= val_loss_min:\n",
    "        tqdm.write(f'\\t\\tval_loss decreased ({val_loss_min:.4f} --> {val_loss:.4f}) saving model...')\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'n_hidden': model.n_hidden,\n",
    "            'n_layers': model.n_layers,\n",
    "            'tokens': model.chars,\n",
    "            'model': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'scheduler': scheduler.state_dict()\n",
    "            }\n",
    "        torch.save(checkpoint, f'drive/MyDrive/char_rnn_ckpt.pth')\n",
    "        val_loss_min = val_loss\n"
   ]
  }
 ]
}